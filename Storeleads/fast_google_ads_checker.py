#!/usr/bin/env python3
"""
é«˜é€Ÿå¹¶å‘Googleå¹¿å‘ŠæŸ¥è¯¢å·¥å…·
ä½¿ç”¨Playwrightå¼‚æ­¥å¹¶å‘ + å¤šæµè§ˆå™¨ä¸Šä¸‹æ–‡ + ç¼“å­˜æœºåˆ¶
é¢„è®¡é€Ÿåº¦ï¼š100åŸŸåçº¦1-2åˆ†é’Ÿ
"""

import asyncio
import pandas as pd
import json
import time
from datetime import datetime
from pathlib import Path
import hashlib

# å°è¯•å¯¼å…¥playwright
try:
    from playwright.async_api import async_playwright
except ImportError:
    print("âŒ Playwrightæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
    import subprocess
    subprocess.run(["pip3", "install", "playwright"], check=True)
    subprocess.run(["playwright", "install", "chromium"], check=True)
    from playwright.async_api import async_playwright

# é…ç½®
CONCURRENT_BROWSERS = 5  # å¹¶å‘æµè§ˆå™¨æ•°é‡ï¼ˆ5ä¸ªæ¯”è¾ƒç¨³å®šï¼‰
TIMEOUT = 15000  # é¡µé¢è¶…æ—¶15ç§’
CACHE_FILE = 'ads_cache.json'

# ç¼“å­˜ç®¡ç†
class AdsCache:
    def __init__(self, cache_file=CACHE_FILE):
        self.cache_file = cache_file
        self.cache = self._load_cache()

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if Path(self.cache_file).exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def get(self, domain):
        """è·å–ç¼“å­˜"""
        return self.cache.get(domain)

    def set(self, domain, data):
        """è®¾ç½®ç¼“å­˜"""
        self.cache[domain] = {
            'data': data,
            'timestamp': datetime.now().isoformat()
        }
        self._save_cache()

    def is_fresh(self, domain, max_age_days=7):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æ–°é²œï¼ˆé»˜è®¤7å¤©ï¼‰"""
        if domain not in self.cache:
            return False
        cached_time = datetime.fromisoformat(self.cache[domain]['timestamp'])
        age = (datetime.now() - cached_time).days
        return age < max_age_days


async def check_single_domain(page, domain):
    """æ£€æŸ¥å•ä¸ªåŸŸåçš„å¹¿å‘Šï¼ˆå¼‚æ­¥ï¼‰"""
    try:
        # å¯¼èˆªåˆ°é¡µé¢
        await page.goto(
            f'https://adstransparency.google.com/?region=anywhere&domain={domain}',
            timeout=TIMEOUT,
            wait_until='domcontentloaded'  # ä¸ç­‰å¾…å®Œå…¨åŠ è½½ï¼Œåªç­‰DOMåŠ è½½
        )

        # å¿«é€Ÿç­‰å¾…å…³é”®å…ƒç´ 
        try:
            await page.wait_for_selector('text=/ä¸ªå¹¿å‘Š/', timeout=3000)
        except:
            # å¦‚æœ3ç§’å†…æ²¡æ‰¾åˆ°ï¼Œè®¤ä¸ºæ²¡æœ‰å¹¿å‘Š
            return {
                'domain': domain,
                'has_ads': False,
                'ad_count': 0,
                'ad_count_text': '0 ä¸ªå¹¿å‘Š',
                'error': None
            }

        # æå–å¹¿å‘Šæ•°é‡
        ad_count_element = await page.query_selector('generic:has-text("ä¸ªå¹¿å‘Š")')
        if ad_count_element:
            ad_count_text = await ad_count_element.inner_text()

            # åˆ¤æ–­æ˜¯å¦æœ‰å¹¿å‘Š
            has_ads = '0 ä¸ªå¹¿å‘Š' not in ad_count_text and 'æœªæ‰¾åˆ°ä»»ä½•å¹¿å‘Š' not in ad_count_text

            # æå–æ•°å­—
            ad_count = 0
            if has_ads:
                if '~' in ad_count_text:
                    # ~200 ä¸ªå¹¿å‘Š
                    ad_count = int(ad_count_text.split('~')[1].split(' ')[0])
                elif ad_count_text[0].isdigit():
                    # 42 ä¸ªå¹¿å‘Š
                    ad_count = int(ad_count_text.split(' ')[0])

            return {
                'domain': domain,
                'has_ads': has_ads,
                'ad_count': ad_count,
                'ad_count_text': ad_count_text,
                'error': None
            }
        else:
            return {
                'domain': domain,
                'has_ads': False,
                'ad_count': 0,
                'ad_count_text': 'æœªæ‰¾åˆ°',
                'error': None
            }

    except Exception as e:
        return {
            'domain': domain,
            'has_ads': False,
            'ad_count': 0,
            'ad_count_text': 'Error',
            'error': str(e)
        }


async def check_domains_batch(domains, progress_callback=None):
    """æ‰¹é‡æ£€æŸ¥åŸŸåï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰"""
    cache = AdsCache()
    results = []
    to_check = []

    # æ£€æŸ¥ç¼“å­˜
    print(f"ğŸ“¦ æ£€æŸ¥ç¼“å­˜...")
    for domain in domains:
        if cache.is_fresh(domain):
            cached = cache.get(domain)
            results.append(cached['data'])
            print(f"  âœ… ç¼“å­˜å‘½ä¸­: {domain}")
        else:
            to_check.append(domain)

    if not to_check:
        print(f"âœ… å…¨éƒ¨æ¥è‡ªç¼“å­˜ï¼")
        return results

    print(f"\nğŸš€ éœ€è¦æŸ¥è¯¢: {len(to_check)} ä¸ªåŸŸå")
    print(f"âš¡ å¹¶å‘æ•°: {CONCURRENT_BROWSERS} ä¸ªæµè§ˆå™¨")

    # å¯åŠ¨Playwright
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼Œæ›´å¿«ï¼‰
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )

        # åˆ›å»ºå¤šä¸ªæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹Ÿå¤šä¸ªç‹¬ç«‹æµè§ˆå™¨ï¼‰
        contexts = []
        for _ in range(CONCURRENT_BROWSERS):
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            contexts.append(context)

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        tasks = []
        context_idx = 0

        for domain in to_check:
            # è½®æµåˆ†é…åˆ°ä¸åŒçš„context
            context = contexts[context_idx % CONCURRENT_BROWSERS]
            page = await context.new_page()

            # åˆ›å»ºä»»åŠ¡
            task = asyncio.create_task(check_single_domain(page, domain))
            tasks.append((task, page))

            context_idx += 1

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        print(f"\nâ³ æŸ¥è¯¢ä¸­...\n")
        completed = 0
        total = len(tasks)

        for task, page in tasks:
            try:
                result = await task
                results.append(result)

                # ä¿å­˜åˆ°ç¼“å­˜
                cache.set(result['domain'], result)

                completed += 1
                status = 'âœ…' if result['has_ads'] else 'â­•'
                print(f"[{completed}/{total}] {status} {result['domain']}: {result['ad_count_text']}")

                if progress_callback:
                    progress_callback(completed, total)

            except Exception as e:
                print(f"âŒ {page.url}: {str(e)}")
            finally:
                await page.close()

        # å…³é—­æµè§ˆå™¨
        for context in contexts:
            await context.close()
        await browser.close()

    return results


async def main():
    """ä¸»å‡½æ•°"""
    print("="*100)
    print("ğŸš€ é«˜é€ŸGoogleå¹¿å‘Šæ‰¹é‡æŸ¥è¯¢å·¥å…·")
    print("="*100)

    # è¯»å–æ•°æ®
    df = pd.read_csv('hangzhou_stores_20k_200k.csv')
    domains = [row['domain'].replace('www.', '') for _, row in df.iterrows()]

    print(f"\nğŸ“Š æ€»å…± {len(domains)} ä¸ªåŸŸå")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # æ‰¹é‡æŸ¥è¯¢
    results = await check_domains_batch(domains)

    # è®¡ç®—è€—æ—¶
    elapsed = time.time() - start_time

    print(f"\n{'='*100}")
    print(f"â±ï¸  æŸ¥è¯¢å®Œæˆï¼æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {elapsed/len(domains):.2f}ç§’/åŸŸå")
    print(f"{'='*100}")

    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    has_ads = sum(1 for r in results if r['has_ads'])
    no_ads = len(results) - has_ads
    total_ads = sum(r['ad_count'] for r in results)

    print(f"  âœ… æœ‰å¹¿å‘Š: {has_ads} ä¸ª")
    print(f"  â­• æ— å¹¿å‘Š: {no_ads} ä¸ª")
    print(f"  ğŸ“Š å¹¿å‘Šæ€»æ•°: {total_ads} ä¸ª")

    # ä¿å­˜ç»“æœ
    df_results = pd.DataFrame(results)

    # åˆå¹¶åŸå§‹æ•°æ®
    df_merged = df.copy()
    df_merged['domain_clean'] = df_merged['domain'].str.replace('www.', '')

    for idx, row in df_merged.iterrows():
        domain_clean = row['domain_clean']
        result = next((r for r in results if r['domain'] == domain_clean), None)
        if result:
            df_merged.at[idx, 'has_ads'] = 'âœ…' if result['has_ads'] else 'âŒ'
            df_merged.at[idx, 'ad_count'] = result['ad_count']
            df_merged.at[idx, 'ad_count_text'] = result['ad_count_text']

    df_merged.to_csv('fast_ads_check_results.csv', index=False, encoding='utf-8-sig')

    # ä¿å­˜JSON
    with open('fast_ads_check_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
    print(f"  - fast_ads_check_results.csv")
    print(f"  - fast_ads_check_results.json")
    print(f"  - ads_cache.json (ç¼“å­˜æ–‡ä»¶)")

    # æ˜¾ç¤ºæœ‰å¹¿å‘Šçš„åº—é“º
    print(f"\n{'='*100}")
    print(f"âœ… æœ‰å¹¿å‘Šçš„åº—é“º:")
    print(f"{'='*100}")
    for r in sorted(results, key=lambda x: x['ad_count'], reverse=True):
        if r['has_ads']:
            print(f"  ğŸ¯ {r['domain']}: {r['ad_count_text']}")


if __name__ == '__main__':
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
