#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS to Database
å°†Hop Skip Mediaåšå®¢ä¿å­˜åˆ°SQLiteæ•°æ®åº“ï¼Œå¹¶è®°å½•è¯¦ç»†æ—¥å¿—
"""

import feedparser
import requests
import sqlite3
import json
from datetime import datetime
from bs4 import BeautifulSoup
import logging
import time

# ==================== é…ç½®åŒº ====================

# æ•°æ®åº“æ–‡ä»¶
DB_FILE = "hopskip_blog.db"

# RSS Feed URL
RSS_FEED_URL = "https://hopskipmedia.com/category/google-adwords/feed/"

# æ—¥å¿—æ–‡ä»¶
LOG_FILE = f"rss_database_{datetime.now().strftime('%Y%m%d')}.log"

# æ˜¯å¦æŠ“å–å®Œæ•´æ–‡ç« å†…å®¹ï¼ˆå¯èƒ½æ¯”è¾ƒæ…¢ï¼‰
FETCH_FULL_CONTENT = True

# ==================== æ—¥å¿—é…ç½® ====================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# ==================== æ•°æ®åº“å‡½æ•° ====================

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    logger.info("ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“...")

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    cursor.execute('''
    CREATE TABLE IF NOT EXISTS hopskip_articles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        article_id TEXT UNIQUE NOT NULL,
        title TEXT NOT NULL,
        url TEXT NOT NULL,
        summary TEXT,
        content TEXT,
        author TEXT,
        published_date DATETIME,
        categories TEXT,
        tags TEXT,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        discord_sent BOOLEAN DEFAULT FALSE,
        discord_sent_at DATETIME
    )
    ''')

    # åˆ›å»ºç´¢å¼•
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_article_id ON hopskip_articles(article_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_published_date ON hopskip_articles(published_date)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_discord_sent ON hopskip_articles(discord_sent)')

    conn.commit()

    # ç»Ÿè®¡ä¿¡æ¯
    cursor.execute('SELECT COUNT(*) FROM hopskip_articles')
    count = cursor.fetchone()[0]
    logger.info(f"âœ… æ•°æ®åº“å·²å°±ç»ªï¼Œå½“å‰æœ‰ {count} ç¯‡æ–‡ç« ")

    conn.close()


def fetch_full_article(url):
    """æŠ“å–æ–‡ç« å®Œæ•´å†…å®¹"""
    try:
        logger.info(f"  ğŸ“¥ æŠ“å–å®Œæ•´å†…å®¹: {url}")
        response = requests.get(url, timeout=15)

        if response.status_code != 200:
            logger.warning(f"  âš ï¸ HTTP {response.status_code}")
            return None

        soup = BeautifulSoup(response.content, 'html.parser')

        # æå–æ–‡ç« å†…å®¹
        article = soup.find('article')
        if article:
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for elem in article.find_all(['script', 'style', 'nav', 'footer', 'header']):
                elem.decompose()

            content = article.get_text(separator='\n', strip=True)
            logger.info(f"  âœ… æŠ“å–æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content
        else:
            logger.warning(f"  âš ï¸ æœªæ‰¾åˆ°articleæ ‡ç­¾")
            return None

    except requests.Timeout:
        logger.error(f"  âŒ è¯·æ±‚è¶…æ—¶")
        return None
    except Exception as e:
        logger.error(f"  âŒ æŠ“å–å¤±è´¥: {e}")
        return None


def save_article(article):
    """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
    article_id = article.get('id', article.get('link'))
    title = article.get('title', 'No Title')
    url = article.get('link', '')
    summary = BeautifulSoup(article.get('summary', ''), 'html.parser').get_text().strip()
    author = article.get('author', 'Unknown')
    published = article.get('published', '')

    logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ç« : {title}")
    logger.info(f"  ğŸ“ URL: {url}")
    logger.info(f"  âœï¸ ä½œè€…: {author}")

    # è§£æå‘å¸ƒæ—¥æœŸ
    try:
        # RSSæ—¥æœŸæ ¼å¼ï¼šMon, 13 Jan 2025 12:00:00 +0000
        published_date = datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
        logger.info(f"  ğŸ“… å‘å¸ƒ: {published_date.strftime('%Y-%m-%d %H:%M:%S')}")
    except:
        published_date = datetime.now()
        logger.warning(f"  âš ï¸ æ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")

    # æå–åˆ†ç±»å’Œæ ‡ç­¾
    categories = []
    tags = []
    for tag in article.get('tags', []):
        term = tag.get('term', '')
        scheme = tag.get('scheme', '')

        if 'category' in scheme.lower():
            categories.append(term)
        else:
            tags.append(term)

    logger.info(f"  ğŸ“‚ åˆ†ç±»: {', '.join(categories) if categories else 'æ— '}")
    logger.info(f"  ğŸ·ï¸ æ ‡ç­¾: {', '.join(tags) if tags else 'æ— '}")

    # æŠ“å–å®Œæ•´å†…å®¹
    content = None
    if FETCH_FULL_CONTENT:
        content = fetch_full_article(url)
        time.sleep(1)  # é¿å…è¯·æ±‚å¤ªé¢‘ç¹

    # ä¿å­˜åˆ°æ•°æ®åº“
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    try:
        cursor.execute('''
        INSERT INTO hopskip_articles
        (article_id, title, url, summary, content, author, published_date, categories, tags)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            article_id,
            title,
            url,
            summary,
            content,
            author,
            published_date,
            json.dumps(categories, ensure_ascii=False),
            json.dumps(tags, ensure_ascii=False)
        ))

        conn.commit()
        logger.info(f"âœ… ä¿å­˜æˆåŠŸ (ID: {cursor.lastrowid})")
        return True

    except sqlite3.IntegrityError:
        logger.info(f"â­ï¸  æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return False

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        conn.rollback()
        return False

    finally:
        conn.close()


def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    stats = {}

    # æ€»æ–‡ç« æ•°
    cursor.execute('SELECT COUNT(*) FROM hopskip_articles')
    stats['total'] = cursor.fetchone()[0]

    # ä»Šå¤©æ–°å¢
    cursor.execute('''
    SELECT COUNT(*) FROM hopskip_articles
    WHERE DATE(created_at) = DATE('now')
    ''')
    stats['today'] = cursor.fetchone()[0]

    # å·²å‘é€åˆ°Discord
    cursor.execute('SELECT COUNT(*) FROM hopskip_articles WHERE discord_sent = TRUE')
    stats['discord_sent'] = cursor.fetchone()[0]

    # æœ€æ–°æ–‡ç« 
    cursor.execute('''
    SELECT title, published_date FROM hopskip_articles
    ORDER BY published_date DESC
    LIMIT 1
    ''')
    row = cursor.fetchone()
    if row:
        stats['latest_title'] = row[0]
        stats['latest_date'] = row[1]
    else:
        stats['latest_title'] = 'N/A'
        stats['latest_date'] = 'N/A'

    conn.close()
    return stats


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 100)
    logger.info("ğŸš€ RSS to Database å¯åŠ¨")
    logger.info(f"ğŸ“¡ RSS Feed: {RSS_FEED_URL}")
    logger.info(f"ğŸ—„ï¸ æ•°æ®åº“: {DB_FILE}")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
    logger.info(f"ğŸ“„ æŠ“å–å®Œæ•´å†…å®¹: {'æ˜¯' if FETCH_FULL_CONTENT else 'å¦'}")
    logger.info("=" * 100)

    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()

    # è§£æRSS Feed
    logger.info("\nğŸ“¡ æ­£åœ¨æŠ“å–RSS Feed...")
    try:
        feed = feedparser.parse(RSS_FEED_URL)
    except Exception as e:
        logger.error(f"âŒ RSSè§£æå¤±è´¥: {e}")
        return

    if not feed.entries:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°RSSæ¡ç›®")
        return

    logger.info(f"ğŸ“° æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« \n")

    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    new_count = 0
    for idx, entry in enumerate(feed.entries, 1):
        logger.info(f"\n{'=' * 80}")
        logger.info(f"[{idx}/{len(feed.entries)}] å¤„ç†æ–‡ç« ")
        logger.info(f"{'=' * 80}")

        if save_article(entry):
            new_count += 1

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "=" * 100)
    logger.info("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    logger.info("=" * 100)

    stats = get_database_stats()
    logger.info(f"æ€»æ–‡ç« æ•°: {stats['total']}")
    logger.info(f"ä»Šæ—¥æ–°å¢: {stats['today']}")
    logger.info(f"å·²å‘é€Discord: {stats['discord_sent']}")
    logger.info(f"æœ€æ–°æ–‡ç« : {stats['latest_title']}")
    logger.info(f"å‘å¸ƒæ—¥æœŸ: {stats['latest_date']}")

    logger.info("\n" + "=" * 100)
    logger.info(f"âœ… å¤„ç†å®Œæˆï¼")
    logger.info(f"   æ–°å¢æ–‡ç« : {new_count}")
    logger.info(f"   è·³è¿‡é‡å¤: {len(feed.entries) - new_count}")
    logger.info("=" * 100)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
